{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import fileinput\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import urllib\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from contractions import CONTRACTION_MAP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from classifier import csv2listdict\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from preprocess import preprocess\n",
    "from clean_text import get_text_sanitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(filenames):\n",
    "    # https://stackoverflow.com/questions/24754861/unicode-file-with-python-and-fileinput\n",
    "    # fileinput.input(filename, openhook=fileinput.hook_encoded(\"utf-8\")).\n",
    "    # raw = url.read().decode('windows-1252')\n",
    "    tweets = []\n",
    "    with fileinput.input((filenames)) as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweet['text_tokenized'] = preprocess(tweet['text'])\n",
    "            tweets.append(tweet)\n",
    "        return tweets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/MelbourneTweets0.txt', \n",
    "             'data/MelbourneTweets2.txt']\n",
    "tweets = read_tweets(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Source code from https://github.com/dipanjanS/text-analytics-with-python\n",
    "# def expand_contractions(sentence, contraction_mapping):\n",
    "#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "#     def expand_match(contraction):\n",
    "#         match = contraction.group(0)\n",
    "#         first_char = match[0]\n",
    "#         expanded_contraction = contraction_mapping.get(match)\\\n",
    "#                                 if contraction_mapping.get(match)\\\n",
    "#                                 else contraction_mapping.get(match.lower())\n",
    "#         expanded_contraction = first_char+expanded_contraction[1:]\n",
    "#         return expanded_contraction\n",
    "\n",
    "#     expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "#     return expanded_sentence\n",
    "\n",
    "# for t in tweets:\n",
    "#     # https://stackoverflow.com/questions/7395789/replacing-a-weird-single-quote-with-blank-string-in-python\n",
    "#     t['text_expanded'] = expand_contractions(t['text'], CONTRACTION_MAP)\n",
    "#     t['text_removed'] = re.sub(r'http\\S+', '', t['text_expanded'])\n",
    "#     t['text_removed'] = re.sub(r'\\n', '', t['text_removed'])\n",
    "#     t['text_removed'] = re.sub(r'RT', '', t['text_removed'])\n",
    "\n",
    "# # this isn't working due to encoding issue\n",
    "# # more on this later\n",
    "# print(expand_contractions(\"next time someone asks why i'm moving i'll reply\", CONTRACTION_MAP))\n",
    "# print(tweets[3]['text'])\n",
    "# expand_contractions(tweets[3]['text'], CONTRACTION_MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handles @mention, make lowercase\n",
    "# tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False, \n",
    "#                                             strip_handles=True, \n",
    "#                                             reduce_len=True)\n",
    "\n",
    "# for t in tweets:\n",
    "#     t['text_tokenized'] = tknzr.tokenize(t['text_removed'])\n",
    "# # tweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets_minus_escape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# tweets_minus_stop = []\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for t in tweets:\n",
    "    not_a_stopword = []\n",
    "    for word in t['text_tokenized']:\n",
    "        word = lemmatize(word)\n",
    "        if word not in stopword_list:\n",
    "            not_a_stopword.append(word)\n",
    "    t['text_tokens'] = not_a_stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "for t in tweets:\n",
    "    for x in set(t['text_tokens']):\n",
    "        token_counter[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# have to get encoding right to resolve this issue\n",
    "# tweets_final = []\n",
    "# for tweet in tweets_minus_stop:\n",
    "#     tweet = [''.join(c for c in s if c not in string.punctuation) for s in tweet]\n",
    "#     tweet = [t for t in tweet if t]\n",
    "#     tweets_final.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion_file = './data/text_emotion.csv'\n",
    "features = ['tweet_id', 'sentiment', 'text']\n",
    "tweets_emotions = pd.read_csv(tweets_emotion_file)\n",
    "# tweets_emotion = csv2listdict(tweets_emotion_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>sanitized_content</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants hang friends soon</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want trade someone houston tickets one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                Funeral ceremony...gloomy friday...   \n",
       "3               wants to hang out with friends SOON!   \n",
       "4  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                   sanitized_content  sentiment_num  \n",
       "0  know listenin bad habit earlier started freaki...              9  \n",
       "1            layin n bed headache ughhhh waitin call              3  \n",
       "2                     funeral ceremony gloomy friday              3  \n",
       "3                            wants hang friends soon             10  \n",
       "4          we want trade someone houston tickets one              0  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    return filtered_sentence\n",
    "\n",
    "def sanitize_twitter_commands(text):\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return text.lower()\n",
    "\n",
    "sanitizer = lambda x: sanitize_twitter_commands(x)\n",
    "stopwordsremover = lambda x: remove_stopwords(x)\n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['content'].apply(sanitizer)\n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['sanitized_content'].apply(stopwordsremover)\n",
    "tweets_emotions['sentiment_num'] = tweets_emotions.sentiment.map({'neutral':0, \n",
    "                                                                  'worry':1, \n",
    "                                                                  'happiness':2,\n",
    "                                                                  'sadness':3,\n",
    "                                                                  'love':4,\n",
    "                                                                  'surprise':5,\n",
    "                                                                  'fun':6,\n",
    "                                                                  'relief':7,\n",
    "                                                                  'hate':8,\n",
    "                                                                  'empty':9,\n",
    "                                                                  'enthusiasm':10,\n",
    "                                                                  'boredom':11,\n",
    "                                                                  'anger':12\n",
    "                                                                 })\n",
    "tweets_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweets_emotions.content\n",
    "y = tweets_emotions.sentiment_num\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@heatedskates That may be, I still don't like hearing his name so much.   #blackhawks #canucks\n",
      "@heatedskates That may be, I still don't like hearing his name so much.  #canucks\n"
     ]
    }
   ],
   "source": [
    "regex_hashtag = re.compile(r'(?:\\A|\\s)#([a-z]{1,})(?:\\Z|\\s)')\n",
    "print(tweets_emotion[32495]['text'])\n",
    "\n",
    "# not understanding why only one hashtag is deletd\n",
    "def remove_hashtag(tweets, regex):\n",
    "    for t in tweets:\n",
    "        t['text'] = re.sub(regex_hashtag, '', t['text'])\n",
    "#         re.sub(regex_hashtag, '', t['text'])\n",
    "    return tweets\n",
    "\n",
    "remove_hashtag(tweets_emotion, regex_hashtag)\n",
    "print(tweets_emotion[32495]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handles @mention, make lowercase\n",
    "tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False, \n",
    "                                            strip_handles=True, \n",
    "                                            reduce_len=True)\n",
    "for t in tweets_emotion:\n",
    "    t['text_tokenized'] = tknzr.tokenize(t['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'may', 'be', ',', 'i', 'still', \"don't\", 'like', 'hearing', 'his', 'name', 'so', 'much', '.', '#canucks']\n",
      "['that', 'may', 'be', ',', 'i', 'still', \"don't\", 'like', 'hearing', 'his', 'name', 'so', 'much', '.', '#canucks']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "print(tweets_emotion[32495]['text_tokenized'])\n",
    "\n",
    "for tweet in tweets_emotion:\n",
    "    for token in tweet['text_tokenized']:\n",
    "        token = lemmatize(token)\n",
    "\n",
    "print(tweets_emotion[32495]['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for t in tweets_emotion:\n",
    "    not_a_stopword = []\n",
    "    for word in t['text_tokenized']:\n",
    "        word = lemmatize(word)\n",
    "        if word not in stops:\n",
    "            not_a_stopword.append(word)\n",
    "    t['text_tokens'] = not_a_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_id', 'sentiment', 'text', 'text_tokenized', 'text_tokens'])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_emotion[3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x = tweets_emotion\n",
    "y = tweets_emotion['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(tweets_emotion)\n",
    "\n",
    "t1 = int(len(tweets_emotion)*0.8)\n",
    "t2 = t1 + int(len(tweets_emotion)*0.1)\n",
    "             \n",
    "training = tweets_emotion[:t1]\n",
    "development = tweets_emotion[t1:t2]\n",
    "testing = tweets_emotion[t2:]\n",
    "\n",
    "shuffle(training)\n",
    "shuffle(development)\n",
    "shuffle(testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x9515 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47500 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_dtm = vectorizer.fit_transform([t['text'] for t in testing])\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.anive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation \n",
    "\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "\n",
    "def prepare_tweets_data(tweets, feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for tweet in tweets:\n",
    "        feature_dict = feature_extractor(tweet)\n",
    "        feature_matrix.append(feature_dict)\n",
    "        if tweet in positive_tweets:\n",
    "            classifications.append('positive')\n",
    "        else:\n",
    "            classifications.append('negative')\n",
    "    \n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset, classifications\n",
    "\n",
    "\n",
    "def check_accuracy(model, predictions, classifications):\n",
    "    print \"\\n\"+model+\" accuracy\"\n",
    "    print accuracy_score(classifications,predictions)\n",
    "\n",
    "dataset, classifications = prepare_tweets_data(development, get_BOW)\n",
    "n_to_test = range(5,25)\n",
    "clfs = [MultinomialNB(n, True, [0.52, 0.5]) for n in n_to_test]\n",
    "for clf in clfs:\n",
    "    predictions = cross_validation.cross_val_predict(clf, dataset, classifications, cv=10)\n",
    "    check_accuracy(\"naive bayes\", predictions, classifications)\n",
    "    print(clf.get_params())\n",
    "\n",
    "n_to_test = range(1,10)\n",
    "clfs = [LogisticRegression(C=n/float(10)) for n in n_to_test]\n",
    "for clf in clfs:\n",
    "    predictions = cross_validation.cross_val_predict(clf, dataset, classifications, cv=10)\n",
    "    check_accuracy(\"logistic regression\", predictions, classifications)\n",
    "    print(clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print \"accuracy\"\n",
    "    print accuracy_score(classifications,predictions)\n",
    "    print classification_report(classifications,predictions)\n",
    "\n",
    "NBclf = MultinomialNB(12, True, [0.52, 0.5]) \n",
    "LRclf = LogisticRegression(C=0.8)\n",
    "\n",
    "training_X, training_Y = prepare_tweets_data(training, get_BOW)\n",
    "testing_X, testing_Y = prepare_tweets_data(training, get_BOW)\n",
    "\n",
    "NBclf.fit(training_X, training_Y)\n",
    "LRclf.fit(training_X, training_Y)\n",
    "\n",
    "print(\"\\nNaive Bayes\\n\")\n",
    "predictions = NBclf.predict(testing_X)\n",
    "check_results(predictions, testing_Y)\n",
    "\n",
    "\n",
    "print(\"\\nLogisticRegression\\n\")\n",
    "predictions = LRclf.predict(testing_X)\n",
    "check_results(predictions, testing_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
