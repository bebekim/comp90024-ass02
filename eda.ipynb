{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import fileinput\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from preprocess import preprocess\n",
    "from clean_text import get_text_sanitized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_emotion_file = './data/text_emotion.csv'\n",
    "features = ['tweet_id', 'sentiment', 'text']\n",
    "tweets_emotions = pd.read_csv(tweets_emotion_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def sanitize_content(text):\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def nostop_content(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopword_list])\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def lemmatize_content(text):\n",
    "    tmp = []\n",
    "    for word in text.split():\n",
    "        tmp.append(lemmatize(word))\n",
    "    return ' '.join(tmp)\n",
    "\n",
    "def int64toint32(x):\n",
    "    x = x.astype(np.int32)\n",
    "    return x\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()   \n",
    "\n",
    "content_sanitizer = lambda x: sanitize_content(x)\n",
    "content_stopwordsremover = lambda x: nostop_content(x)\n",
    "content_lemmatizer = lambda x: lemmatize_content(x)\n",
    "type_converter = lambda x: int64toint32(x)\n",
    "         \n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['content'].apply(content_sanitizer)\n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['sanitized_content'].apply(content_stopwordsremover)\n",
    "tweets_emotions['lemmatized_content'] = tweets_emotions['sanitized_content'].apply(content_lemmatizer)\n",
    "\n",
    "# tweets_emotions['sentiment_num'] = tweets_emotions.sentiment.map({'neutral':0, \n",
    "#                                                                   'worry':1, \n",
    "#                                                                   'happiness':2,\n",
    "#                                                                   'sadness':3,\n",
    "#                                                                   'love':4,\n",
    "#                                                                   'surprise':5,\n",
    "#                                                                   'fun':6,\n",
    "#                                                                   'relief':7,\n",
    "#                                                                   'hate':8,\n",
    "#                                                                   'empty':9,\n",
    "#                                                                   'enthusiasm':10,\n",
    "#                                                                   'boredom':11,\n",
    "#                                                                   'anger':12\n",
    "#                                                                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "(8000,)\n",
      "(32000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweets_emotions.lemmatized_content\n",
    "y = tweets_emotions.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31337500000000001"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_bow = count_vect.fit(X_train)\n",
    "X_train_bow = count_vect.transform(X_train)\n",
    "X_test_bow = count_vect.transform(X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_bow)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "y_predict_class = clf.predict(X_test_bow)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral       1740\n",
      "worry         1666\n",
      "sadness       1046\n",
      "happiness     1028\n",
      "love           762\n",
      "surprise       425\n",
      "relief         352\n",
      "fun            338\n",
      "hate           268\n",
      "enthusiasm     163\n",
      "empty          162\n",
      "boredom         31\n",
      "anger           19\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tweets(filenames):\n",
    "    # https://stackoverflow.com/questions/24754861/unicode-file-with-python-and-fileinput\n",
    "    # fileinput.input(filename, openhook=fileinput.hook_encoded(\"utf-8\")).\n",
    "    # raw = url.read().decode('windows-1252')\n",
    "    tweets = []\n",
    "    with fileinput.input((filenames)) as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweet['text_tokenized'] = preprocess(tweet['text'])\n",
    "            tweets.append(tweet)\n",
    "        return tweets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/MelbourneTweets0.txt', \n",
    "             'data/MelbourneTweets2.txt']\n",
    "tweets = read_tweets(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handles @mention, make lowercase\n",
    "tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False, \n",
    "                                            strip_handles=True, \n",
    "                                            reduce_len=True)\n",
    "regexp_hashtag = re.compile(r'(?:\\A|\\s)#([a-z]{1,})(?:\\Z|\\s)')\n",
    "regexp_url = re.compile(r\"http\\S+\")\n",
    "\n",
    "\n",
    "for t in tweets:\n",
    "    tokenized = tknzr.tokenize(t['text'])\n",
    "    not_a_stopword = []\n",
    "    for word in tokenized:\n",
    "        word = lemmatize(word)\n",
    "        if word not in stopword_list:\n",
    "            not_a_stopword.append(word)\n",
    "    t['text_tokens'] = ' '.join(not_a_stopword)\n",
    "    predict_new = clf.predict(count_vect.transform([t['text_tokens']]))\n",
    "    t['text_sentiment'] = predict_new.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regex_hashtag = re.compile(r'(?:\\A|\\s)#([a-z]{1,})(?:\\Z|\\s)')\n",
    "# print(tweets_emotion[32495]['text'])\n",
    "\n",
    "# # not understanding why only one hashtag is deletd\n",
    "# def remove_hashtag(tweets, regex):\n",
    "#     for t in tweets:\n",
    "#         t['text'] = re.sub(regex_hashtag, '', t['text'])\n",
    "# #         re.sub(regex_hashtag, '', t['text'])\n",
    "#     return tweets\n",
    "\n",
    "# remove_hashtag(tweets_emotion, regex_hashtag)\n",
    "# print(tweets_emotion[32495]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# have to get encoding right to resolve this issue\n",
    "# tweets_final = []\n",
    "# for tweet in tweets_minus_stop:\n",
    "#     tweet = [''.join(c for c in s if c not in string.punctuation) for s in tweet]\n",
    "#     tweet = [t for t in tweet if t]\n",
    "#     tweets_final.append(tweet)\n",
    "\n",
    "# print(tweets_emotion[32495]['text_tokenized'])\n",
    "\n",
    "# for tweet in tweets_emotion:\n",
    "#     for token in tweet['text_tokenized']:\n",
    "#         token = lemmatize(token)\n",
    "\n",
    "# print(tweets_emotion[32495]['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = tweets[3298]\n",
    "# dict1 = { 'name' : 'song', 'age' : 10 }\n",
    "\n",
    "# print(\"dict1 = {0}\".format(dict1))\n",
    "# print(\"dict1 type = {0}\".format(type(dict1)))\n",
    "# print(\"================\")\n",
    "\n",
    "\n",
    "# CONVERT dictionary to json using json.dump\n",
    "\n",
    "json_val = json.dumps(dict1)\n",
    "tweets_json = json.dumps(tweets)\n",
    "\n",
    "with open('output.json', 'w') as outfile:\n",
    "    json.dump(json_val, outfile)\n",
    "    \n",
    "    \n",
    "json = json.dumps(dict)\n",
    "f = open(\"dict.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
