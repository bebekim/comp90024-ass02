{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import fileinput\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import urllib\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from contractions import CONTRACTION_MAP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from classifier import csv2listdict\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "from preprocess import preprocess\n",
    "from clean_text import get_text_sanitized\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emotion_file = './data/text_emotion.csv'\n",
    "features = ['tweet_id', 'sentiment', 'text']\n",
    "tweets_emotions = pd.read_csv(tweets_emotion_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence).lower()\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def sanitize_content(text):\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def nostop_content(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopword_list])\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def lemmatize_content(text):\n",
    "    tmp = []\n",
    "    for word in text.split():\n",
    "        tmp.append(lemmatize(word))\n",
    "    return ' '.join(tmp)\n",
    "\n",
    "\n",
    "# for t in tweets:\n",
    "#     not_a_stopword = []\n",
    "#     for word in t['text_tokenized']:\n",
    "#         word = lemmatize(word)\n",
    "#         if word not in stopword_list:\n",
    "#             not_a_stopword.append(word)\n",
    "#     t['text_tokens'] = not_a_stopword\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()   \n",
    "\n",
    "content_sanitizer = lambda x: sanitize_content(x)\n",
    "content_stopwordsremover = lambda x: nostop_content(x)\n",
    "content_lemmatizer = lambda x: lemmatize_content(x)\n",
    "\n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['content'].apply(content_sanitizer)\n",
    "tweets_emotions['sanitized_content'] = tweets_emotions['sanitized_content'].apply(content_stopwordsremover)\n",
    "tweets_emotions['lemmatized_content'] = tweets_emotions['sanitized_content'].apply(content_lemmatizer)\n",
    "\n",
    "tweets_emotions['sentiment_num'] = tweets_emotions.sentiment.map({'neutral':0, \n",
    "                                                                  'worry':1, \n",
    "                                                                  'happiness':2,\n",
    "                                                                  'sadness':3,\n",
    "                                                                  'love':4,\n",
    "                                                                  'surprise':5,\n",
    "                                                                  'fun':6,\n",
    "                                                                  'relief':7,\n",
    "                                                                  'hate':8,\n",
    "                                                                  'empty':9,\n",
    "                                                                  'enthusiasm':10,\n",
    "                                                                  'boredom':11,\n",
    "                                                                  'anger':12\n",
    "                                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "(8000,)\n",
      "(32000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweets_emotions.lemmatized_content\n",
    "y = tweets_emotions.sentiment_num\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit(X_train)\n",
    "X_train_bow = vectorizer.transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31962499999999999"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train)\n",
    "\n",
    "# make class predictions for X_test_bow\n",
    "y_predict_class = nb.predict(X_test_bow)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1740\n",
      "1     1666\n",
      "3     1046\n",
      "2     1028\n",
      "4      762\n",
      "5      425\n",
      "7      352\n",
      "6      338\n",
      "8      268\n",
      "10     163\n",
      "9      162\n",
      "11      31\n",
      "12      19\n",
      "Name: sentiment_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(filenames):\n",
    "    # https://stackoverflow.com/questions/24754861/unicode-file-with-python-and-fileinput\n",
    "    # fileinput.input(filename, openhook=fileinput.hook_encoded(\"utf-8\")).\n",
    "    # raw = url.read().decode('windows-1252')\n",
    "    tweets = []\n",
    "    with fileinput.input((filenames)) as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweet['text_tokenized'] = preprocess(tweet['text'])\n",
    "            tweets.append(tweet)\n",
    "        return tweets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/MelbourneTweets0.txt', \n",
    "             'data/MelbourneTweets2.txt']\n",
    "tweets = read_tweets(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Source code from https://github.com/dipanjanS/text-analytics-with-python\n",
    "# def expand_contractions(sentence, contraction_mapping):\n",
    "#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "#     def expand_match(contraction):\n",
    "#         match = contraction.group(0)\n",
    "#         first_char = match[0]\n",
    "#         expanded_contraction = contraction_mapping.get(match)\\\n",
    "#                                 if contraction_mapping.get(match)\\\n",
    "#                                 else contraction_mapping.get(match.lower())\n",
    "#         expanded_contraction = first_char+expanded_contraction[1:]\n",
    "#         return expanded_contraction\n",
    "\n",
    "#     expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "#     return expanded_sentence\n",
    "\n",
    "# for t in tweets:\n",
    "#     # https://stackoverflow.com/questions/7395789/replacing-a-weird-single-quote-with-blank-string-in-python\n",
    "#     t['text_expanded'] = expand_contractions(t['text'], CONTRACTION_MAP)\n",
    "#     t['text_removed'] = re.sub(r'http\\S+', '', t['text_expanded'])\n",
    "#     t['text_removed'] = re.sub(r'\\n', '', t['text_removed'])\n",
    "#     t['text_removed'] = re.sub(r'RT', '', t['text_removed'])\n",
    "\n",
    "# # this isn't working due to encoding issue\n",
    "# # more on this later\n",
    "# print(expand_contractions(\"next time someone asks why i'm moving i'll reply\", CONTRACTION_MAP))\n",
    "# print(tweets[3]['text'])\n",
    "# expand_contractions(tweets[3]['text'], CONTRACTION_MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handles @mention, make lowercase\n",
    "# tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False, \n",
    "#                                             strip_handles=True, \n",
    "#                                             reduce_len=True)\n",
    "\n",
    "# for t in tweets:\n",
    "#     t['text_tokenized'] = tknzr.tokenize(t['text_removed'])\n",
    "# # tweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets_minus_escape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tweets_minus_stop = []\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for t in tweets:\n",
    "    not_a_stopword = []\n",
    "    for word in t['text_tokenized']:\n",
    "        word = lemmatize(word)\n",
    "        if word not in stopword_list:\n",
    "            not_a_stopword.append(word)\n",
    "    t['text_tokens'] = not_a_stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "for t in tweets:\n",
    "    for x in set(t['text_tokens']):\n",
    "        token_counter[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# have to get encoding right to resolve this issue\n",
    "# tweets_final = []\n",
    "# for tweet in tweets_minus_stop:\n",
    "#     tweet = [''.join(c for c in s if c not in string.punctuation) for s in tweet]\n",
    "#     tweet = [t for t in tweet if t]\n",
    "#     tweets_final.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@heatedskates That may be, I still don't like hearing his name so much.   #blackhawks #canucks\n",
      "@heatedskates That may be, I still don't like hearing his name so much.  #canucks\n"
     ]
    }
   ],
   "source": [
    "regex_hashtag = re.compile(r'(?:\\A|\\s)#([a-z]{1,})(?:\\Z|\\s)')\n",
    "print(tweets_emotion[32495]['text'])\n",
    "\n",
    "# not understanding why only one hashtag is deletd\n",
    "def remove_hashtag(tweets, regex):\n",
    "    for t in tweets:\n",
    "        t['text'] = re.sub(regex_hashtag, '', t['text'])\n",
    "#         re.sub(regex_hashtag, '', t['text'])\n",
    "    return tweets\n",
    "\n",
    "remove_hashtag(tweets_emotion, regex_hashtag)\n",
    "print(tweets_emotion[32495]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handles @mention, make lowercase\n",
    "tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False, \n",
    "                                            strip_handles=True, \n",
    "                                            reduce_len=True)\n",
    "for t in tweets_emotion:\n",
    "    t['text_tokenized'] = tknzr.tokenize(t['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(tweets_emotion[32495]['text_tokenized'])\n",
    "\n",
    "# for tweet in tweets_emotion:\n",
    "#     for token in tweet['text_tokenized']:\n",
    "#         token = lemmatize(token)\n",
    "\n",
    "# print(tweets_emotion[32495]['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for t in tweets_emotion:\n",
    "    not_a_stopword = []\n",
    "    for word in t['text_tokenized']:\n",
    "        word = lemmatize(word)\n",
    "        if word not in stops:\n",
    "            not_a_stopword.append(word)\n",
    "    t['text_tokens'] = not_a_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_id', 'sentiment', 'text', 'text_tokenized', 'text_tokens'])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_emotion[3].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
